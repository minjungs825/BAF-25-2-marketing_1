{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet 기반 음식 분류\n",
        "\n",
        "**EfficientNet 모델**을 활용하여 음식 이미지를 분류하는 과정이다.\n",
        "데이터 전처리, 모델 정의, 학습 및 평가 단계를 포함하며, 이미지 분류 정확도를 향상시키기 위한 다양한 실험을 진행된다.\n",
        "\n",
        "- **주요 내용**  \n",
        "  - 데이터 로드 및 전처리  \n",
        "  - EfficientNet 모델 불러오기 (Pretrained)  \n",
        "  - 분류기 학습 (Fine-tuning)  \n",
        "  - 성능 평가 및 결과 시각화  \n"
      ],
      "metadata": {
        "id": "JLjo_D11uu11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtgUJNDM7Nic",
        "outputId": "df39bd25-4057-45bc-c2b9-1338fa0e34c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8hiclQT-8VZ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 원하는 루트 경로 (여기에 train/val 구조 만들기)\n",
        "root_dir = \"/content/dataset_split_food20_2\"\n",
        "os.makedirs(root_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. 데이터셋 학습/검증 분할\n",
        "\n",
        "- `food_classes` 리스트에 20개 음식 카테고리를 정의\n",
        "- 학습 데이터와 검증 데이터는 **8:2 비율**로 랜덤 분할\n",
        "\n"
      ],
      "metadata": {
        "id": "cjYYYiMivs0O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QUbX6sG8Vce",
        "outputId": "2db1b9b2-0345-45a6-e909-4b02e4d9787d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ImageFolder 구조 세팅 완료: /content/dataset_split_food20_2\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# 음식 클래스 목록\n",
        "food_classes = [\n",
        "    \"감자전\",\"감자탕\",\"과메기\",\"떡볶이\",\"라면\",\"만두\",\"보쌈\",\"삼계탕\",\n",
        "    \"새우튀김\",\"수제비\",\"순대\",\"양념치킨\",\"육회\",\"족발\",\"짜장면\",\n",
        "    \"짬뽕\",\"콩국수\",\"파전\",\"피자\",\"후라이드치킨\"\n",
        "]\n",
        "\n",
        "# 구글드라이브 zip 파일 경로\n",
        "zip_root = \"/content/drive/MyDrive/\"   # 네가 zip 올려둔 폴더\n",
        "\n",
        "for cname in food_classes:\n",
        "    zip_path = os.path.join(zip_root, f\"{cname}.zip\")\n",
        "    extract_path = os.path.join(root_dir, cname)\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # 압축 해제\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # 이미지 파일 모으기\n",
        "    imgs = []\n",
        "    for root, _, files in os.walk(extract_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n",
        "                imgs.append(os.path.join(root,f))\n",
        "\n",
        "    # train/val 디렉토리 생성\n",
        "    train_out = os.path.join(root_dir, \"train\", cname)\n",
        "    val_out   = os.path.join(root_dir, \"val\", cname)\n",
        "    os.makedirs(train_out, exist_ok=True)\n",
        "    os.makedirs(val_out, exist_ok=True)\n",
        "\n",
        "    # train:val = 8:2 split\n",
        "    random.shuffle(imgs)\n",
        "    split_idx = int(len(imgs) * 0.8)\n",
        "    train_imgs, val_imgs = imgs[:split_idx], imgs[split_idx:]\n",
        "\n",
        "    # 파일 이동\n",
        "    for src in train_imgs:\n",
        "        shutil.move(src, os.path.join(train_out, os.path.basename(src)))\n",
        "    for src in val_imgs:\n",
        "        shutil.move(src, os.path.join(val_out, os.path.basename(src)))\n",
        "\n",
        "    # 원래 압축 풀린 임시 폴더 제거\n",
        "    shutil.rmtree(extract_path, ignore_errors=True)\n",
        "\n",
        "print(\"✅ ImageFolder 구조 세팅 완료:\", root_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiNvR3l28Ve0",
        "outputId": "6e810aa9-602d-4730-a4d3-1875354654a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "클래스: ['감자전', '감자탕', '과메기', '떡볶이', '라면', '만두', '보쌈', '삼계탕', '새우튀김', '수제비', '순대', '양념치킨', '육회', '족발', '짜장면', '짬뽕', '콩국수', '파전', '피자', '후라이드치킨']\n",
            "train 샘플 수: 31999\n",
            "val 샘플 수: 8009\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(root_dir, \"train\"))\n",
        "val_dataset   = datasets.ImageFolder(os.path.join(root_dir, \"val\"))\n",
        "\n",
        "print(\"클래스:\", train_dataset.classes)\n",
        "print(\"train 샘플 수:\", len(train_dataset))\n",
        "print(\"val 샘플 수:\", len(val_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step2. Food-20 통합 파이프라인 (`food20_full.py`)\n",
        "\n",
        "학습:**음식 이미지 분류 + 검증**을 위한 전체 파이프라인 제공\n",
        "\n",
        "- **Utils/전처리**: 데이터 변환, 디바이스 설정, 폴더 생성  \n",
        "- **Feature Extractor & AE**: EfficientNet-B4 특징 벡터 추출 → 클래스별 AutoEncoder 학습/검증  \n",
        "- **Classifier**: EfficientNet-B4 기반 다중 분류기 학습 및 추론 (Top-1, Top-k)  \n",
        "- **Classifier + AE**: 분류기 예측 후 AE로 재검증하여 신뢰도 향상  \n",
        "- **Demo**: 간단한 2-클래스 학습 예시 포함  \n",
        "\n",
        "-> **분류 정확도 + 신뢰성**을 동시에 높이는 구조\n"
      ],
      "metadata": {
        "id": "slSweLgww2SI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqptzQ6x80Cj",
        "outputId": "e6b4c8b5-1ae1-4fdd-eb43-4dc5eb2604e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/food20_full.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/food20_full.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Food-20 Unified Pipeline (Loop-ready: AE + Classifier)\n",
        "\n",
        "이 파일은 다음을 지원\n",
        "1) 20-클래스(또는 N-클래스) 이미지 분류기 학습/추론 (사진 -> 어떤 음식인지)\n",
        "2) 다수 클래스용 원-클래스 AE 반복 학습 (클래스별 '맞는지' 검증용)\n",
        "3) 복합 추론: 분류기 top-k 후보 + 각 후보 AE 재검증(선택)\n",
        "\n",
        "데이터 구조 (ImageFolder)\n",
        "root/\n",
        "  train/\n",
        "    ramen/\n",
        "    jjajang/\n",
        "    bibimbap/\n",
        "    ...\n",
        "  val/\n",
        "    ramen/\n",
        "    jjajang/\n",
        "    bibimbap/\n",
        "    ...\n",
        "\"\"\"\n",
        "\n",
        "import os, json\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Utils / Transforms\n",
        "# ---------------------\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_transforms(img_size: int = 380, is_train: bool = True):\n",
        "    if is_train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(int(img_size * 1.15)),\n",
        "            transforms.CenterCrop(img_size),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(int(img_size * 1.15)),\n",
        "            transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ---------------------\n",
        "# EfficientNet-B4 feature extractor (for AE)\n",
        "# ---------------------\n",
        "def create_effnet_b4_feature_extractor(device: torch.device):\n",
        "    effnet = timm.create_model(\"efficientnet_b4\", pretrained=True)\n",
        "    feat_dim = effnet.classifier.in_features  # 1792\n",
        "    effnet.classifier = nn.Identity()\n",
        "    effnet.to(device).eval()\n",
        "    return effnet, feat_dim\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_features(loader: DataLoader, effnet: nn.Module, device: torch.device):\n",
        "    feats, labels = [], []\n",
        "    for imgs, labs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out = effnet(imgs)             # [B, 1792]\n",
        "        feats.append(out.cpu())\n",
        "        labels.append(labs)\n",
        "    return torch.cat(feats), torch.cat(labels)\n",
        "\n",
        "# ---------------------\n",
        "# One-Class AE (per class)\n",
        "# ---------------------\n",
        "class FeatureAE(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 512, bottleneck_dim: int = 128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, bottleneck_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)\n",
        "\n",
        "def _subset_class(dataset: datasets.ImageFolder, class_idx: int):\n",
        "    idx = [i for i, (_, y) in enumerate(dataset.samples) if y == class_idx]\n",
        "    return torch.utils.data.Subset(dataset, idx)\n",
        "\n",
        "def train_oneclass_ae(\n",
        "    root: str,\n",
        "    class_name: str,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 10,\n",
        "    lr: float = 1e-3,\n",
        "    weights_dir: str = \"weights\",\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    해당 class_name의 이미지(훈련셋만)로 Feature-AE를 학습하고\n",
        "    threshold(mean + 2*std)을 저장/반환.\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    ensure_dir(weights_dir)\n",
        "\n",
        "    train_tf = build_transforms(img_size, True)\n",
        "    train_ds = datasets.ImageFolder(os.path.join(root, \"train\"), transform=train_tf)\n",
        "\n",
        "    if class_name not in train_ds.class_to_idx:\n",
        "        raise ValueError(f\"{class_name} not in {train_ds.classes}\")\n",
        "    class_idx = train_ds.class_to_idx[class_name]\n",
        "\n",
        "    train_subset = _subset_class(train_ds, class_idx)\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    effnet, feat_dim = create_effnet_b4_feature_extractor(device)\n",
        "    train_feats, _ = extract_features(train_loader, effnet, device)\n",
        "\n",
        "    ae = FeatureAE(feat_dim, 1792, 256).to(device)\n",
        "    crit = nn.MSELoss()\n",
        "    opt  = optim.Adam(ae.parameters(), lr=lr)\n",
        "\n",
        "    ds = torch.utils.data.TensorDataset(train_feats, train_feats)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        ae.train()\n",
        "        run = 0.0\n",
        "        for x, _ in dl:\n",
        "            x = x.to(device)\n",
        "            recon = ae(x)\n",
        "            loss = crit(recon, x)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            run += loss.item() * x.size(0)\n",
        "        print(f\"[AE][{class_name}] Epoch {ep+1}/{epochs} | Loss {run/len(dl.dataset):.6f}\")\n",
        "\n",
        "    # threshold from train set\n",
        "    ae.eval()\n",
        "    with torch.no_grad():\n",
        "        recon = ae(train_feats.to(device))\n",
        "        errors = torch.mean((recon - train_feats.to(device))**2, dim=1).cpu().numpy()\n",
        "    threshold = float(np.mean(errors) + 2*np.std(errors))\n",
        "\n",
        "    torch.save({\n",
        "        \"model\": ae.state_dict(),\n",
        "        \"feat_dim\": feat_dim,\n",
        "        \"hidden_dim\": 1792,\n",
        "        \"bottleneck_dim\": 256\n",
        "    }, os.path.join(weights_dir, f\"ae_{class_name}.pth\"))\n",
        "\n",
        "    with open(os.path.join(weights_dir, f\"ae_{class_name}_threshold.txt\"), \"w\") as f:\n",
        "        f.write(str(threshold))\n",
        "    print(f\"[AE][{class_name}] Saved. threshold={threshold:.6f}\")\n",
        "    return threshold\n",
        "\n",
        "def train_many_oneclass_aes(\n",
        "    root: str,\n",
        "    classes: Optional[List[str]] = None,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 10,\n",
        "    lr: float = 1e-3,\n",
        "    weights_dir: str = \"weights\",\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    여러 클래스를 반복문으로 AE 일괄 학습. classes=None이면 train의 모든 클래스 자동.\n",
        "    \"\"\"\n",
        "    train_ds = datasets.ImageFolder(os.path.join(root, \"train\"))\n",
        "    target_classes = classes or train_ds.classes\n",
        "    print(f\"[AE] Target classes: {target_classes}\")\n",
        "    out = {}\n",
        "    for cname in target_classes:\n",
        "        out[cname] = train_oneclass_ae(\n",
        "            root=root, class_name=cname, img_size=img_size,\n",
        "            batch_size=batch_size, epochs=epochs, lr=lr, weights_dir=weights_dir\n",
        "        )\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def is_food_by_ae(\n",
        "    img_path: str,\n",
        "    class_name: str,\n",
        "    img_size: int = 380,\n",
        "    weights_dir: str = \"weights\",\n",
        ") -> Tuple[bool, float, float]:\n",
        "    \"\"\"\n",
        "    해당 class_name의 AE로 재구성오차를 측정, threshold와 비교.\n",
        "    return: (is_class, recon_error, threshold)\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    w_path = os.path.join(weights_dir, f\"ae_{class_name}.pth\")\n",
        "    t_path = os.path.join(weights_dir, f\"ae_{class_name}_threshold.txt\")\n",
        "    if not (os.path.isfile(w_path) and os.path.isfile(t_path)):\n",
        "        raise FileNotFoundError(f\"Missing AE for {class_name}\")\n",
        "\n",
        "    ckpt = torch.load(w_path, map_location=device)\n",
        "    ae = FeatureAE(\n",
        "        input_dim=ckpt[\"feat_dim\"],\n",
        "        hidden_dim=ckpt[\"hidden_dim\"],\n",
        "        bottleneck_dim=ckpt[\"bottleneck_dim\"]\n",
        "    ).to(device)\n",
        "    ae.load_state_dict(ckpt[\"model\"])\n",
        "    ae.eval()\n",
        "\n",
        "\n",
        "    with open(t_path, \"r\") as f:\n",
        "        threshold = float(f.read().strip())\n",
        "\n",
        "    effnet, _ = create_effnet_b4_feature_extractor(device)\n",
        "    tf = build_transforms(img_size, False)\n",
        "    x = tf(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    feat  = effnet(x)\n",
        "    recon = ae(feat)\n",
        "    err = torch.mean((recon - feat)**2).item()\n",
        "    return (err < threshold), float(err), float(threshold)\n",
        "\n",
        "# ---------------------\n",
        "# Multi-Class Classifier (사진 -> 무엇인지)\n",
        "# ---------------------\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, target):\n",
        "        if self.eps == 0.0:\n",
        "            return nn.functional.cross_entropy(logits, target)\n",
        "        n = logits.size(1)\n",
        "        log_probs = self.log_softmax(logits)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(logits)\n",
        "            true_dist.fill_(self.eps / (n - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.eps)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
        "\n",
        "def _maybe_weighted_sampler(ds: datasets.ImageFolder):\n",
        "    counts = np.bincount([y for _, y in ds.samples], minlength=len(ds.classes))\n",
        "    if (counts == 0).any():  # 방어\n",
        "        return None\n",
        "    weights = 1.0 / (counts + 1e-9)\n",
        "    sw = [weights[y] for _, y in ds.samples]\n",
        "    return WeightedRandomSampler(sw, num_samples=len(sw), replacement=True)\n",
        "\n",
        "def _save_labelmap(ds: datasets.ImageFolder, out_path: str):\n",
        "    labelmap = {int(v): k for k, v in ds.class_to_idx.items()}\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(labelmap, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def _load_labelmap(path: str) -> Dict[int, str]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return {int(k): v for k, v in json.load(f).items()}\n",
        "\n",
        "def accuracy_topk(logits: torch.Tensor, target: torch.Tensor, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / target.size(0)))\n",
        "    return res\n",
        "\n",
        "def train_food_classifier(\n",
        "    root: str,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    label_smoothing: float = 0.0,\n",
        "    freeze_backbone: bool = False,\n",
        "    patience: int = 3,\n",
        "    weights_dir: str = \"weights\",\n",
        "):\n",
        "    \"\"\"\n",
        "    N-클래스 음식 분류기 학습 (EfficientNet-B4 finetune). best 가중치 자동 저장.\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    ensure_dir(weights_dir)\n",
        "\n",
        "    train_tf = build_transforms(img_size, True)\n",
        "    val_tf   = build_transforms(img_size, False)\n",
        "\n",
        "    train_ds = datasets.ImageFolder(os.path.join(root, \"train\"), transform=train_tf)\n",
        "    val_ds   = datasets.ImageFolder(os.path.join(root, \"val\"), transform=val_tf)\n",
        "    num_classes = len(train_ds.classes)\n",
        "    print(f\"[CLS] Classes({num_classes}): {train_ds.classes}\")\n",
        "\n",
        "    labelmap_path = os.path.join(weights_dir, \"food_labelmap.json\")\n",
        "    _save_labelmap(train_ds, labelmap_path)\n",
        "    print(f\"[CLS] Saved label map -> {labelmap_path}\")\n",
        "\n",
        "    sampler = _maybe_weighted_sampler(train_ds)\n",
        "    if sampler is None:\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes=num_classes)\n",
        "    if freeze_backbone:\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"classifier\" not in name:\n",
        "                p.requires_grad = False\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = LabelSmoothingCE(eps=label_smoothing)\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs))\n",
        "\n",
        "    best_acc, best_path = 0.0, os.path.join(weights_dir, \"cls_food_best.pth\")\n",
        "    no_improve = 0\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        tc = 0; tt = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tc += (out.argmax(1) == y).sum().item()\n",
        "            tt += x.size(0)\n",
        "        scheduler.step()\n",
        "        train_acc = tc / max(1, tt)\n",
        "\n",
        "        model.eval()\n",
        "        vc = 0; vt = 0\n",
        "        t1s, t3s = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                vc += (out.argmax(1) == y).sum().item()\n",
        "                vt += x.size(0)\n",
        "                t1, t3 = accuracy_topk(out, y, topk=(1,3))\n",
        "                t1s.append(t1.item()); t3s.append(t3.item())\n",
        "        val_acc  = vc / max(1, vt)\n",
        "        val_t1   = float(np.mean(t1s)) if t1s else 0.0\n",
        "        val_t3   = float(np.mean(t3s)) if t3s else 0.0\n",
        "        print(f\"[CLS] Epoch {ep+1:02d}/{epochs} | Train {train_acc:.4f} || Val {val_acc:.4f} Top1 {val_t1:.2f}% Top3 {val_t3:.2f}%\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save({\"model\": model.state_dict(), \"num_classes\": num_classes}, best_path)\n",
        "            print(f\"[CLS] Saved best -> {best_path} (Acc {best_acc:.4f})\")\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(f\"[CLS] Early stopping (patience={patience})\")\n",
        "                break\n",
        "\n",
        "    print(f\"[CLS] Best Val Acc: {best_acc:.4f} | Weights: {best_path}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_food(\n",
        "    img_path: str,\n",
        "    model_path: str = \"weights/cls_food_best.pth\",\n",
        "    labelmap_path: str = \"weights/food_labelmap.json\",\n",
        "    img_size: int = 380,\n",
        "    topk: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    분류기 추론 (사진 -> top-1, top-k 확률).\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    if not os.path.isfile(model_path):\n",
        "        raise FileNotFoundError(model_path)\n",
        "    if not os.path.isfile(labelmap_path):\n",
        "        raise FileNotFoundError(labelmap_path)\n",
        "\n",
        "    labelmap = _load_labelmap(labelmap_path)\n",
        "    num_classes = len(labelmap)\n",
        "\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=False, num_classes=num_classes).to(device)\n",
        "    ckpt = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "    tf = build_transforms(img_size, False)\n",
        "    x = tf(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "\n",
        "    logits = model(x)\n",
        "    probs  = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
        "    k = min(topk, num_classes)\n",
        "    idx = np.argsort(-probs)[:k].tolist()\n",
        "    topk_pairs = [(labelmap[i], float(probs[i])) for i in idx]\n",
        "    return {\"top1\": topk_pairs[0], \"topk\": topk_pairs}\n",
        "\n",
        "# ---------------------\n",
        "# Classifier -> AE verify combo\n",
        "# ---------------------\n",
        "@torch.no_grad()\n",
        "def predict_food_then_verify(\n",
        "    img_path: str,\n",
        "    model_path: str = \"weights/cls_food_best.pth\",\n",
        "    labelmap_path: str = \"weights/food_labelmap.json\",\n",
        "    img_size: int = 380,\n",
        "    topk: int = 3,\n",
        "    verify_with_ae: bool = True,\n",
        "    weights_dir: str = \"weights\",\n",
        "):\n",
        "    \"\"\"\n",
        "    1) 분류기 top-k 후보 예측\n",
        "    2) (옵션) 각 후보 클래스에 대해 AE 재구성오차로 검증\n",
        "    \"\"\"\n",
        "    pred = predict_food(img_path, model_path, labelmap_path, img_size, topk)\n",
        "    if not verify_with_ae:\n",
        "        return {\"pred\": pred, \"ae_checks\": None}\n",
        "\n",
        "    checks = []\n",
        "    for cname, prob in pred[\"topk\"]:\n",
        "        try:\n",
        "            ok, err, th = is_food_by_ae(img_path, cname, img_size, weights_dir)\n",
        "            checks.append({\"class\": cname, \"prob\": prob, \"ae_ok\": bool(ok), \"recon_error\": err, \"threshold\": th})\n",
        "        except FileNotFoundError:\n",
        "            checks.append({\"class\": cname, \"prob\": prob, \"ae_ok\": None, \"recon_error\": None, \"threshold\": None})\n",
        "    return {\"pred\": pred, \"ae_checks\": checks}\n",
        "\n",
        "# ---------------------\n",
        "#기존 2-클래스 데모도 유지\n",
        "# ---------------------\n",
        "def quick_two_class_demo(\n",
        "    data_dir: str,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 3,\n",
        "    lr: float = 1e-4,\n",
        "):\n",
        "    device = get_device()\n",
        "    train_tf = build_transforms(img_size, True)\n",
        "    val_tf   = build_transforms(img_size, False)\n",
        "\n",
        "    train_ds = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_tf)\n",
        "    val_ds   = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=val_tf)\n",
        "    assert len(train_ds.classes) == 2, \"Need exactly 2 classes for this demo.\"\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes=2).to(device)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt  = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train(); tc=0; tt=0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad(); out = model(x); loss = crit(out, y)\n",
        "            loss.backward(); opt.step()\n",
        "            tc += (out.argmax(1) == y).sum().item()\n",
        "            tt += x.size(0)\n",
        "\n",
        "        model.eval(); vc=0; vt=0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                vc += (out.argmax(1) == y).sum().item()\n",
        "                vt += x.size(0)\n",
        "\n",
        "        print(f\"[2C] Epoch {ep+1}/{epochs} | Train {tc/max(1,tt):.4f} | Val {vc/max(1,vt):.4f}\")\n",
        "    print(\"[2C] Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kaa3Rud80Fb",
        "outputId": "b47bc954-4f9e-4e91-beda-803ea752ee55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 제거된 Mac 리소스 포크 파일 수: 20004\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "\n",
        "ROOT = \"/content/dataset_split_food20_2\"\n",
        "bad_files = []\n",
        "\n",
        "for f in glob.glob(ROOT + \"/**/._*\", recursive=True):\n",
        "    bad_files.append(f)\n",
        "    os.remove(f)\n",
        "\n",
        "print(\"✅ 제거된 Mac 리소스 포크 파일 수:\", len(bad_files))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KJjH3Yi880IC"
      },
      "outputs": [],
      "source": [
        "import sys, importlib\n",
        "\n",
        "# food20_full이 이미 로드되어 있으면 강제로 리로드\n",
        "if 'food20_full' in sys.modules:\n",
        "    importlib.reload(sys.modules['food20_full'])\n",
        "else:\n",
        "    import food20_full\n",
        "\n",
        "import food20_full as f  # 앞으로 f. 으로 호출할 거에요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step3. 결과 출력\n",
        "- 짜장면 이미지 데이터를 활용해 검증"
      ],
      "metadata": {
        "id": "6xjAL7ttzNDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "59144bec41244afd95ebc273e41a6504",
            "d86c489cae864cdbbb4623bde98d5d69",
            "e6baa69370934dedb05d75943ae57a75",
            "2595bbc7e3a846788b661bd948370a55",
            "73e8dd7f9ab4479fa6cc27d481c7b800",
            "3d39e9cea7f6405497cdebba9e9c9397",
            "036e068e29e140acb3852457f067a04b",
            "f0a53c14e9f7454087c501916b4efe45",
            "d56274cb75ae4421a9856b10f183e388",
            "8147fc0b6f5542f297ffc621540c0582",
            "16a713bd46014433994bb405eb7dbff3"
          ]
        },
        "id": "Qbh49Cd_-KxD",
        "outputId": "66d6de95-efde-4075-910f-74876d97c4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Step 1. 20-class 분류기 학습 시작\n",
            "[CLS] Classes(20): ['감자전', '감자탕', '과메기', '떡볶이', '라면', '만두', '보쌈', '삼계탕', '새우튀김', '수제비', '순대', '양념치킨', '육회', '족발', '짜장면', '짬뽕', '콩국수', '파전', '피자', '후라이드치킨']\n",
            "[CLS] Saved label map -> weights/food_labelmap.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59144bec41244afd95ebc273e41a6504"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 01/5 | Train 0.7035 || Val 0.8764 Top1 87.64% Top3 97.25%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.8764)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 02/5 | Train 0.9325 || Val 0.9112 Top1 91.12% Top3 98.22%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.9112)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 03/5 | Train 0.9627 || Val 0.9262 Top1 92.62% Top3 98.47%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.9262)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 04/5 | Train 0.9746 || Val 0.9277 Top1 92.78% Top3 98.42%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.9277)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 05/5 | Train 0.9796 || Val 0.9257 Top1 92.57% Top3 98.50%\n",
            "[CLS] Best Val Acc: 0.9277 | Weights: weights/cls_food_best.pth\n",
            "\n",
            "🚀 Step 2. 클래스별 AE 학습 시작\n",
            "[AE] Target classes: ['감자전', '감자탕', '과메기', '떡볶이', '라면', '만두', '보쌈', '삼계탕', '새우튀김', '수제비', '순대', '양념치킨', '육회', '족발', '짜장면', '짬뽕', '콩국수', '파전', '피자', '후라이드치킨']\n",
            "[AE][감자전] Epoch 1/3 | Loss 0.004726\n",
            "[AE][감자전] Epoch 2/3 | Loss 0.003791\n",
            "[AE][감자전] Epoch 3/3 | Loss 0.003294\n",
            "[AE][감자전] Saved. threshold=0.005620\n",
            "[AE][감자탕] Epoch 1/3 | Loss 0.004063\n",
            "[AE][감자탕] Epoch 2/3 | Loss 0.003042\n",
            "[AE][감자탕] Epoch 3/3 | Loss 0.002736\n",
            "[AE][감자탕] Saved. threshold=0.004669\n",
            "[AE][과메기] Epoch 1/3 | Loss 0.006760\n",
            "[AE][과메기] Epoch 2/3 | Loss 0.004999\n",
            "[AE][과메기] Epoch 3/3 | Loss 0.004382\n",
            "[AE][과메기] Saved. threshold=0.006524\n",
            "[AE][떡볶이] Epoch 1/3 | Loss 0.004569\n",
            "[AE][떡볶이] Epoch 2/3 | Loss 0.003449\n",
            "[AE][떡볶이] Epoch 3/3 | Loss 0.002877\n",
            "[AE][떡볶이] Saved. threshold=0.004529\n",
            "[AE][라면] Epoch 1/3 | Loss 0.004176\n",
            "[AE][라면] Epoch 2/3 | Loss 0.003154\n",
            "[AE][라면] Epoch 3/3 | Loss 0.002731\n",
            "[AE][라면] Saved. threshold=0.004838\n",
            "[AE][만두] Epoch 1/3 | Loss 0.005522\n",
            "[AE][만두] Epoch 2/3 | Loss 0.004303\n",
            "[AE][만두] Epoch 3/3 | Loss 0.003681\n",
            "[AE][만두] Saved. threshold=0.005870\n",
            "[AE][보쌈] Epoch 1/3 | Loss 0.005454\n",
            "[AE][보쌈] Epoch 2/3 | Loss 0.004211\n",
            "[AE][보쌈] Epoch 3/3 | Loss 0.003579\n",
            "[AE][보쌈] Saved. threshold=0.004900\n",
            "[AE][삼계탕] Epoch 1/3 | Loss 0.004044\n",
            "[AE][삼계탕] Epoch 2/3 | Loss 0.003053\n",
            "[AE][삼계탕] Epoch 3/3 | Loss 0.002611\n",
            "[AE][삼계탕] Saved. threshold=0.004550\n",
            "[AE][새우튀김] Epoch 1/3 | Loss 0.005832\n",
            "[AE][새우튀김] Epoch 2/3 | Loss 0.004497\n",
            "[AE][새우튀김] Epoch 3/3 | Loss 0.003809\n",
            "[AE][새우튀김] Saved. threshold=0.005755\n",
            "[AE][수제비] Epoch 1/3 | Loss 0.004023\n",
            "[AE][수제비] Epoch 2/3 | Loss 0.003126\n",
            "[AE][수제비] Epoch 3/3 | Loss 0.002634\n",
            "[AE][수제비] Saved. threshold=0.004315\n",
            "[AE][순대] Epoch 1/3 | Loss 0.006040\n",
            "[AE][순대] Epoch 2/3 | Loss 0.004509\n",
            "[AE][순대] Epoch 3/3 | Loss 0.003784\n",
            "[AE][순대] Saved. threshold=0.005452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][양념치킨] Epoch 1/3 | Loss 0.004652\n",
            "[AE][양념치킨] Epoch 2/3 | Loss 0.003453\n",
            "[AE][양념치킨] Epoch 3/3 | Loss 0.002877\n",
            "[AE][양념치킨] Saved. threshold=0.004694\n",
            "[AE][육회] Epoch 1/3 | Loss 0.005792\n",
            "[AE][육회] Epoch 2/3 | Loss 0.004336\n",
            "[AE][육회] Epoch 3/3 | Loss 0.003556\n",
            "[AE][육회] Saved. threshold=0.005325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][족발] Epoch 1/3 | Loss 0.004970\n",
            "[AE][족발] Epoch 2/3 | Loss 0.003750\n",
            "[AE][족발] Epoch 3/3 | Loss 0.003118\n",
            "[AE][족발] Saved. threshold=0.004530\n",
            "[AE][짜장면] Epoch 1/3 | Loss 0.005458\n",
            "[AE][짜장면] Epoch 2/3 | Loss 0.003703\n",
            "[AE][짜장면] Epoch 3/3 | Loss 0.003009\n",
            "[AE][짜장면] Saved. threshold=0.004651\n",
            "[AE][짬뽕] Epoch 1/3 | Loss 0.003411\n",
            "[AE][짬뽕] Epoch 2/3 | Loss 0.002409\n",
            "[AE][짬뽕] Epoch 3/3 | Loss 0.002082\n",
            "[AE][짬뽕] Saved. threshold=0.003677\n",
            "[AE][콩국수] Epoch 1/3 | Loss 0.004975\n",
            "[AE][콩국수] Epoch 2/3 | Loss 0.003930\n",
            "[AE][콩국수] Epoch 3/3 | Loss 0.003356\n",
            "[AE][콩국수] Saved. threshold=0.005261\n",
            "[AE][파전] Epoch 1/3 | Loss 0.004219\n",
            "[AE][파전] Epoch 2/3 | Loss 0.003178\n",
            "[AE][파전] Epoch 3/3 | Loss 0.002673\n",
            "[AE][파전] Saved. threshold=0.004471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][피자] Epoch 1/3 | Loss 0.002910\n",
            "[AE][피자] Epoch 2/3 | Loss 0.002235\n",
            "[AE][피자] Epoch 3/3 | Loss 0.001972\n",
            "[AE][피자] Saved. threshold=0.004085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][후라이드치킨] Epoch 1/3 | Loss 0.004538\n",
            "[AE][후라이드치킨] Epoch 2/3 | Loss 0.003426\n",
            "[AE][후라이드치킨] Epoch 3/3 | Loss 0.002987\n",
            "[AE][후라이드치킨] Saved. threshold=0.004971\n",
            "클래스별 Thresholds: {'감자전': 0.005619973875582218, '감자탕': 0.004669167101383209, '과메기': 0.006523739546537399, '떡볶이': 0.004529417492449284, '라면': 0.004837975837290287, '만두': 0.005870405584573746, '보쌈': 0.004900071769952774, '삼계탕': 0.004549554083496332, '새우튀김': 0.0057545071467757225, '수제비': 0.004315010737627745, '순대': 0.005451973527669907, '양념치킨': 0.004694024566560984, '육회': 0.005324722267687321, '족발': 0.004530000500380993, '짜장면': 0.00465080002322793, '짬뽕': 0.003676858264952898, '콩국수': 0.005260597914457321, '파전': 0.004470800515264273, '피자': 0.004085250664502382, '후라이드치킨': 0.0049706390127539635}\n",
            "\n",
            "🚀 Step 3. 추론 실행\n",
            "\n",
            "최종 결과:\n",
            "{'pred': {'top1': ('짜장면', 0.8835874795913696), 'topk': [('짜장면', 0.8835874795913696), ('후라이드치킨', 0.012351472862064838), ('감자전', 0.010002527385950089)]}, 'ae_checks': [{'class': '짜장면', 'prob': 0.8835874795913696, 'ae_ok': True, 'recon_error': 0.001568456063978374, 'threshold': 0.00465080002322793}, {'class': '후라이드치킨', 'prob': 0.012351472862064838, 'ae_ok': False, 'recon_error': 0.006881140638142824, 'threshold': 0.0049706390127539635}, {'class': '감자전', 'prob': 0.010002527385950089, 'ae_ok': False, 'recon_error': 0.005914350505918264, 'threshold': 0.005619973875582218}]}\n"
          ]
        }
      ],
      "source": [
        "import sys, importlib\n",
        "\n",
        "# 모듈 리로드\n",
        "if \"food20_full\" in sys.modules:\n",
        "    importlib.reload(sys.modules[\"food20_full\"])\n",
        "else:\n",
        "    import food20_full\n",
        "\n",
        "import food20_full as f   # 앞으로 f. 으로 호출\n",
        "\n",
        "# 1. 데이터 루트 경로\n",
        "ROOT = \"/content/dataset_split_food20_2\"\n",
        "\n",
        "# 2. 분류기 학습\n",
        "print(\"🚀 Step 1. 20-class 분류기 학습 시작\")\n",
        "f.train_food_classifier(\n",
        "    root=ROOT,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        "    label_smoothing=0.1,\n",
        "    freeze_backbone=False\n",
        ")\n",
        "\n",
        "# 3. 클래스별 AE 학습\n",
        "print(\"\\n🚀 Step 2. 클래스별 AE 학습 시작\")\n",
        "thresholds = f.train_many_oneclass_aes(\n",
        "    root=ROOT,\n",
        "    epochs=3\n",
        ")\n",
        "print(\"클래스별 Thresholds:\", thresholds)\n",
        "\n",
        "# 4. 추론 테스트\n",
        "print(\"\\n🚀 Step 3. 추론 실행\")\n",
        "test_img = \"/content/common-9.jpg\"\n",
        "result = f.predict_food_then_verify(\n",
        "    img_path=test_img,\n",
        "    model_path=\"weights/cls_food_best.pth\",\n",
        "    labelmap_path=\"weights/food_labelmap.json\",\n",
        "    img_size=380,\n",
        "    topk=3,\n",
        "    verify_with_ae=True\n",
        ")\n",
        "\n",
        "print(\"\\n최종 결과:\")\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59144bec41244afd95ebc273e41a6504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d86c489cae864cdbbb4623bde98d5d69",
              "IPY_MODEL_e6baa69370934dedb05d75943ae57a75",
              "IPY_MODEL_2595bbc7e3a846788b661bd948370a55"
            ],
            "layout": "IPY_MODEL_73e8dd7f9ab4479fa6cc27d481c7b800"
          }
        },
        "d86c489cae864cdbbb4623bde98d5d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d39e9cea7f6405497cdebba9e9c9397",
            "placeholder": "​",
            "style": "IPY_MODEL_036e068e29e140acb3852457f067a04b",
            "value": "model.safetensors: 100%"
          }
        },
        "e6baa69370934dedb05d75943ae57a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a53c14e9f7454087c501916b4efe45",
            "max": 77933206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d56274cb75ae4421a9856b10f183e388",
            "value": 77933206
          }
        },
        "2595bbc7e3a846788b661bd948370a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8147fc0b6f5542f297ffc621540c0582",
            "placeholder": "​",
            "style": "IPY_MODEL_16a713bd46014433994bb405eb7dbff3",
            "value": " 77.9M/77.9M [00:00&lt;00:00, 93.9MB/s]"
          }
        },
        "73e8dd7f9ab4479fa6cc27d481c7b800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d39e9cea7f6405497cdebba9e9c9397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "036e068e29e140acb3852457f067a04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0a53c14e9f7454087c501916b4efe45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56274cb75ae4421a9856b10f183e388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8147fc0b6f5542f297ffc621540c0582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16a713bd46014433994bb405eb7dbff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}