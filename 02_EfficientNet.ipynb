{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet ê¸°ë°˜ ìŒì‹ ë¶„ë¥˜\n",
        "\n",
        "**EfficientNet ëª¨ë¸**ì„ í™œìš©í•˜ì—¬ ìŒì‹ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n",
        "ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ ì •ì˜, í•™ìŠµ ë° í‰ê°€ ë‹¨ê³„ë¥¼ í¬í•¨í•˜ë©°, ì´ë¯¸ì§€ ë¶„ë¥˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰ëœë‹¤.\n",
        "\n",
        "- **ì£¼ìš” ë‚´ìš©**  \n",
        "  - ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬  \n",
        "  - EfficientNet ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (Pretrained)  \n",
        "  - ë¶„ë¥˜ê¸° í•™ìŠµ (Fine-tuning)  \n",
        "  - ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ ì‹œê°í™”  \n"
      ],
      "metadata": {
        "id": "JLjo_D11uu11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtgUJNDM7Nic",
        "outputId": "df39bd25-4057-45bc-c2b9-1338fa0e34c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8hiclQT-8VZ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ì›í•˜ëŠ” ë£¨íŠ¸ ê²½ë¡œ (ì—¬ê¸°ì— train/val êµ¬ì¡° ë§Œë“¤ê¸°)\n",
        "root_dir = \"/content/dataset_split_food20_2\"\n",
        "os.makedirs(root_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. ë°ì´í„°ì…‹ í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
        "\n",
        "- `food_classes` ë¦¬ìŠ¤íŠ¸ì— 20ê°œ ìŒì‹ ì¹´í…Œê³ ë¦¬ë¥¼ ì •ì˜\n",
        "- í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ëŠ” **8:2 ë¹„ìœ¨**ë¡œ ëœë¤ ë¶„í• \n",
        "\n"
      ],
      "metadata": {
        "id": "cjYYYiMivs0O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QUbX6sG8Vce",
        "outputId": "2db1b9b2-0345-45a6-e909-4b02e4d9787d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ImageFolder êµ¬ì¡° ì„¸íŒ… ì™„ë£Œ: /content/dataset_split_food20_2\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# ìŒì‹ í´ë˜ìŠ¤ ëª©ë¡\n",
        "food_classes = [\n",
        "    \"ê°ìì „\",\"ê°ìíƒ•\",\"ê³¼ë©”ê¸°\",\"ë–¡ë³¶ì´\",\"ë¼ë©´\",\"ë§Œë‘\",\"ë³´ìŒˆ\",\"ì‚¼ê³„íƒ•\",\n",
        "    \"ìƒˆìš°íŠ€ê¹€\",\"ìˆ˜ì œë¹„\",\"ìˆœëŒ€\",\"ì–‘ë…ì¹˜í‚¨\",\"ìœ¡íšŒ\",\"ì¡±ë°œ\",\"ì§œì¥ë©´\",\n",
        "    \"ì§¬ë½•\",\"ì½©êµ­ìˆ˜\",\"íŒŒì „\",\"í”¼ì\",\"í›„ë¼ì´ë“œì¹˜í‚¨\"\n",
        "]\n",
        "\n",
        "# êµ¬ê¸€ë“œë¼ì´ë¸Œ zip íŒŒì¼ ê²½ë¡œ\n",
        "zip_root = \"/content/drive/MyDrive/\"   # ë„¤ê°€ zip ì˜¬ë ¤ë‘” í´ë”\n",
        "\n",
        "for cname in food_classes:\n",
        "    zip_path = os.path.join(zip_root, f\"{cname}.zip\")\n",
        "    extract_path = os.path.join(root_dir, cname)\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # ì••ì¶• í•´ì œ\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # ì´ë¯¸ì§€ íŒŒì¼ ëª¨ìœ¼ê¸°\n",
        "    imgs = []\n",
        "    for root, _, files in os.walk(extract_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n",
        "                imgs.append(os.path.join(root,f))\n",
        "\n",
        "    # train/val ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "    train_out = os.path.join(root_dir, \"train\", cname)\n",
        "    val_out   = os.path.join(root_dir, \"val\", cname)\n",
        "    os.makedirs(train_out, exist_ok=True)\n",
        "    os.makedirs(val_out, exist_ok=True)\n",
        "\n",
        "    # train:val = 8:2 split\n",
        "    random.shuffle(imgs)\n",
        "    split_idx = int(len(imgs) * 0.8)\n",
        "    train_imgs, val_imgs = imgs[:split_idx], imgs[split_idx:]\n",
        "\n",
        "    # íŒŒì¼ ì´ë™\n",
        "    for src in train_imgs:\n",
        "        shutil.move(src, os.path.join(train_out, os.path.basename(src)))\n",
        "    for src in val_imgs:\n",
        "        shutil.move(src, os.path.join(val_out, os.path.basename(src)))\n",
        "\n",
        "    # ì›ë˜ ì••ì¶• í’€ë¦° ì„ì‹œ í´ë” ì œê±°\n",
        "    shutil.rmtree(extract_path, ignore_errors=True)\n",
        "\n",
        "print(\"âœ… ImageFolder êµ¬ì¡° ì„¸íŒ… ì™„ë£Œ:\", root_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiNvR3l28Ve0",
        "outputId": "6e810aa9-602d-4730-a4d3-1875354654a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í´ë˜ìŠ¤: ['ê°ìì „', 'ê°ìíƒ•', 'ê³¼ë©”ê¸°', 'ë–¡ë³¶ì´', 'ë¼ë©´', 'ë§Œë‘', 'ë³´ìŒˆ', 'ì‚¼ê³„íƒ•', 'ìƒˆìš°íŠ€ê¹€', 'ìˆ˜ì œë¹„', 'ìˆœëŒ€', 'ì–‘ë…ì¹˜í‚¨', 'ìœ¡íšŒ', 'ì¡±ë°œ', 'ì§œì¥ë©´', 'ì§¬ë½•', 'ì½©êµ­ìˆ˜', 'íŒŒì „', 'í”¼ì', 'í›„ë¼ì´ë“œì¹˜í‚¨']\n",
            "train ìƒ˜í”Œ ìˆ˜: 31999\n",
            "val ìƒ˜í”Œ ìˆ˜: 8009\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(root_dir, \"train\"))\n",
        "val_dataset   = datasets.ImageFolder(os.path.join(root_dir, \"val\"))\n",
        "\n",
        "print(\"í´ë˜ìŠ¤:\", train_dataset.classes)\n",
        "print(\"train ìƒ˜í”Œ ìˆ˜:\", len(train_dataset))\n",
        "print(\"val ìƒ˜í”Œ ìˆ˜:\", len(val_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step2. Food-20 í†µí•© íŒŒì´í”„ë¼ì¸ (`food20_full.py`)\n",
        "\n",
        "í•™ìŠµ:**ìŒì‹ ì´ë¯¸ì§€ ë¶„ë¥˜ + ê²€ì¦**ì„ ìœ„í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì œê³µ\n",
        "\n",
        "- **Utils/ì „ì²˜ë¦¬**: ë°ì´í„° ë³€í™˜, ë””ë°”ì´ìŠ¤ ì„¤ì •, í´ë” ìƒì„±  \n",
        "- **Feature Extractor & AE**: EfficientNet-B4 íŠ¹ì§• ë²¡í„° ì¶”ì¶œ â†’ í´ë˜ìŠ¤ë³„ AutoEncoder í•™ìŠµ/ê²€ì¦  \n",
        "- **Classifier**: EfficientNet-B4 ê¸°ë°˜ ë‹¤ì¤‘ ë¶„ë¥˜ê¸° í•™ìŠµ ë° ì¶”ë¡  (Top-1, Top-k)  \n",
        "- **Classifier + AE**: ë¶„ë¥˜ê¸° ì˜ˆì¸¡ í›„ AEë¡œ ì¬ê²€ì¦í•˜ì—¬ ì‹ ë¢°ë„ í–¥ìƒ  \n",
        "- **Demo**: ê°„ë‹¨í•œ 2-í´ë˜ìŠ¤ í•™ìŠµ ì˜ˆì‹œ í¬í•¨  \n",
        "\n",
        "-> **ë¶„ë¥˜ ì •í™•ë„ + ì‹ ë¢°ì„±**ì„ ë™ì‹œì— ë†’ì´ëŠ” êµ¬ì¡°\n"
      ],
      "metadata": {
        "id": "slSweLgww2SI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqptzQ6x80Cj",
        "outputId": "e6b4c8b5-1ae1-4fdd-eb43-4dc5eb2604e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/food20_full.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/food20_full.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Food-20 Unified Pipeline (Loop-ready: AE + Classifier)\n",
        "\n",
        "ì´ íŒŒì¼ì€ ë‹¤ìŒì„ ì§€ì›\n",
        "1) 20-í´ë˜ìŠ¤(ë˜ëŠ” N-í´ë˜ìŠ¤) ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° í•™ìŠµ/ì¶”ë¡  (ì‚¬ì§„ -> ì–´ë–¤ ìŒì‹ì¸ì§€)\n",
        "2) ë‹¤ìˆ˜ í´ë˜ìŠ¤ìš© ì›-í´ë˜ìŠ¤ AE ë°˜ë³µ í•™ìŠµ (í´ë˜ìŠ¤ë³„ 'ë§ëŠ”ì§€' ê²€ì¦ìš©)\n",
        "3) ë³µí•© ì¶”ë¡ : ë¶„ë¥˜ê¸° top-k í›„ë³´ + ê° í›„ë³´ AE ì¬ê²€ì¦(ì„ íƒ)\n",
        "\n",
        "ë°ì´í„° êµ¬ì¡° (ImageFolder)\n",
        "root/\n",
        "  train/\n",
        "    ramen/\n",
        "    jjajang/\n",
        "    bibimbap/\n",
        "    ...\n",
        "  val/\n",
        "    ramen/\n",
        "    jjajang/\n",
        "    bibimbap/\n",
        "    ...\n",
        "\"\"\"\n",
        "\n",
        "import os, json\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Utils / Transforms\n",
        "# ---------------------\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_transforms(img_size: int = 380, is_train: bool = True):\n",
        "    if is_train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(int(img_size * 1.15)),\n",
        "            transforms.CenterCrop(img_size),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(int(img_size * 1.15)),\n",
        "            transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ---------------------\n",
        "# EfficientNet-B4 feature extractor (for AE)\n",
        "# ---------------------\n",
        "def create_effnet_b4_feature_extractor(device: torch.device):\n",
        "    effnet = timm.create_model(\"efficientnet_b4\", pretrained=True)\n",
        "    feat_dim = effnet.classifier.in_features  # 1792\n",
        "    effnet.classifier = nn.Identity()\n",
        "    effnet.to(device).eval()\n",
        "    return effnet, feat_dim\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_features(loader: DataLoader, effnet: nn.Module, device: torch.device):\n",
        "    feats, labels = [], []\n",
        "    for imgs, labs in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out = effnet(imgs)             # [B, 1792]\n",
        "        feats.append(out.cpu())\n",
        "        labels.append(labs)\n",
        "    return torch.cat(feats), torch.cat(labels)\n",
        "\n",
        "# ---------------------\n",
        "# One-Class AE (per class)\n",
        "# ---------------------\n",
        "class FeatureAE(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 512, bottleneck_dim: int = 128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, bottleneck_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)\n",
        "\n",
        "def _subset_class(dataset: datasets.ImageFolder, class_idx: int):\n",
        "    idx = [i for i, (_, y) in enumerate(dataset.samples) if y == class_idx]\n",
        "    return torch.utils.data.Subset(dataset, idx)\n",
        "\n",
        "def train_oneclass_ae(\n",
        "    root: str,\n",
        "    class_name: str,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 10,\n",
        "    lr: float = 1e-3,\n",
        "    weights_dir: str = \"weights\",\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    í•´ë‹¹ class_nameì˜ ì´ë¯¸ì§€(í›ˆë ¨ì…‹ë§Œ)ë¡œ Feature-AEë¥¼ í•™ìŠµí•˜ê³ \n",
        "    threshold(mean + 2*std)ì„ ì €ì¥/ë°˜í™˜.\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    ensure_dir(weights_dir)\n",
        "\n",
        "    train_tf = build_transforms(img_size, True)\n",
        "    train_ds = datasets.ImageFolder(os.path.join(root, \"train\"), transform=train_tf)\n",
        "\n",
        "    if class_name not in train_ds.class_to_idx:\n",
        "        raise ValueError(f\"{class_name} not in {train_ds.classes}\")\n",
        "    class_idx = train_ds.class_to_idx[class_name]\n",
        "\n",
        "    train_subset = _subset_class(train_ds, class_idx)\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    effnet, feat_dim = create_effnet_b4_feature_extractor(device)\n",
        "    train_feats, _ = extract_features(train_loader, effnet, device)\n",
        "\n",
        "    ae = FeatureAE(feat_dim, 1792, 256).to(device)\n",
        "    crit = nn.MSELoss()\n",
        "    opt  = optim.Adam(ae.parameters(), lr=lr)\n",
        "\n",
        "    ds = torch.utils.data.TensorDataset(train_feats, train_feats)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        ae.train()\n",
        "        run = 0.0\n",
        "        for x, _ in dl:\n",
        "            x = x.to(device)\n",
        "            recon = ae(x)\n",
        "            loss = crit(recon, x)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            run += loss.item() * x.size(0)\n",
        "        print(f\"[AE][{class_name}] Epoch {ep+1}/{epochs} | Loss {run/len(dl.dataset):.6f}\")\n",
        "\n",
        "    # threshold from train set\n",
        "    ae.eval()\n",
        "    with torch.no_grad():\n",
        "        recon = ae(train_feats.to(device))\n",
        "        errors = torch.mean((recon - train_feats.to(device))**2, dim=1).cpu().numpy()\n",
        "    threshold = float(np.mean(errors) + 2*np.std(errors))\n",
        "\n",
        "    torch.save({\n",
        "        \"model\": ae.state_dict(),\n",
        "        \"feat_dim\": feat_dim,\n",
        "        \"hidden_dim\": 1792,\n",
        "        \"bottleneck_dim\": 256\n",
        "    }, os.path.join(weights_dir, f\"ae_{class_name}.pth\"))\n",
        "\n",
        "    with open(os.path.join(weights_dir, f\"ae_{class_name}_threshold.txt\"), \"w\") as f:\n",
        "        f.write(str(threshold))\n",
        "    print(f\"[AE][{class_name}] Saved. threshold={threshold:.6f}\")\n",
        "    return threshold\n",
        "\n",
        "def train_many_oneclass_aes(\n",
        "    root: str,\n",
        "    classes: Optional[List[str]] = None,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 10,\n",
        "    lr: float = 1e-3,\n",
        "    weights_dir: str = \"weights\",\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ í´ë˜ìŠ¤ë¥¼ ë°˜ë³µë¬¸ìœ¼ë¡œ AE ì¼ê´„ í•™ìŠµ. classes=Noneì´ë©´ trainì˜ ëª¨ë“  í´ë˜ìŠ¤ ìë™.\n",
        "    \"\"\"\n",
        "    train_ds = datasets.ImageFolder(os.path.join(root, \"train\"))\n",
        "    target_classes = classes or train_ds.classes\n",
        "    print(f\"[AE] Target classes: {target_classes}\")\n",
        "    out = {}\n",
        "    for cname in target_classes:\n",
        "        out[cname] = train_oneclass_ae(\n",
        "            root=root, class_name=cname, img_size=img_size,\n",
        "            batch_size=batch_size, epochs=epochs, lr=lr, weights_dir=weights_dir\n",
        "        )\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def is_food_by_ae(\n",
        "    img_path: str,\n",
        "    class_name: str,\n",
        "    img_size: int = 380,\n",
        "    weights_dir: str = \"weights\",\n",
        ") -> Tuple[bool, float, float]:\n",
        "    \"\"\"\n",
        "    í•´ë‹¹ class_nameì˜ AEë¡œ ì¬êµ¬ì„±ì˜¤ì°¨ë¥¼ ì¸¡ì •, thresholdì™€ ë¹„êµ.\n",
        "    return: (is_class, recon_error, threshold)\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    w_path = os.path.join(weights_dir, f\"ae_{class_name}.pth\")\n",
        "    t_path = os.path.join(weights_dir, f\"ae_{class_name}_threshold.txt\")\n",
        "    if not (os.path.isfile(w_path) and os.path.isfile(t_path)):\n",
        "        raise FileNotFoundError(f\"Missing AE for {class_name}\")\n",
        "\n",
        "    ckpt = torch.load(w_path, map_location=device)\n",
        "    ae = FeatureAE(\n",
        "        input_dim=ckpt[\"feat_dim\"],\n",
        "        hidden_dim=ckpt[\"hidden_dim\"],\n",
        "        bottleneck_dim=ckpt[\"bottleneck_dim\"]\n",
        "    ).to(device)\n",
        "    ae.load_state_dict(ckpt[\"model\"])\n",
        "    ae.eval()\n",
        "\n",
        "\n",
        "    with open(t_path, \"r\") as f:\n",
        "        threshold = float(f.read().strip())\n",
        "\n",
        "    effnet, _ = create_effnet_b4_feature_extractor(device)\n",
        "    tf = build_transforms(img_size, False)\n",
        "    x = tf(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    feat  = effnet(x)\n",
        "    recon = ae(feat)\n",
        "    err = torch.mean((recon - feat)**2).item()\n",
        "    return (err < threshold), float(err), float(threshold)\n",
        "\n",
        "# ---------------------\n",
        "# Multi-Class Classifier (ì‚¬ì§„ -> ë¬´ì—‡ì¸ì§€)\n",
        "# ---------------------\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, target):\n",
        "        if self.eps == 0.0:\n",
        "            return nn.functional.cross_entropy(logits, target)\n",
        "        n = logits.size(1)\n",
        "        log_probs = self.log_softmax(logits)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(logits)\n",
        "            true_dist.fill_(self.eps / (n - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.eps)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
        "\n",
        "def _maybe_weighted_sampler(ds: datasets.ImageFolder):\n",
        "    counts = np.bincount([y for _, y in ds.samples], minlength=len(ds.classes))\n",
        "    if (counts == 0).any():  # ë°©ì–´\n",
        "        return None\n",
        "    weights = 1.0 / (counts + 1e-9)\n",
        "    sw = [weights[y] for _, y in ds.samples]\n",
        "    return WeightedRandomSampler(sw, num_samples=len(sw), replacement=True)\n",
        "\n",
        "def _save_labelmap(ds: datasets.ImageFolder, out_path: str):\n",
        "    labelmap = {int(v): k for k, v in ds.class_to_idx.items()}\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(labelmap, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def _load_labelmap(path: str) -> Dict[int, str]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return {int(k): v for k, v in json.load(f).items()}\n",
        "\n",
        "def accuracy_topk(logits: torch.Tensor, target: torch.Tensor, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / target.size(0)))\n",
        "    return res\n",
        "\n",
        "def train_food_classifier(\n",
        "    root: str,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    label_smoothing: float = 0.0,\n",
        "    freeze_backbone: bool = False,\n",
        "    patience: int = 3,\n",
        "    weights_dir: str = \"weights\",\n",
        "):\n",
        "    \"\"\"\n",
        "    N-í´ë˜ìŠ¤ ìŒì‹ ë¶„ë¥˜ê¸° í•™ìŠµ (EfficientNet-B4 finetune). best ê°€ì¤‘ì¹˜ ìë™ ì €ì¥.\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    ensure_dir(weights_dir)\n",
        "\n",
        "    train_tf = build_transforms(img_size, True)\n",
        "    val_tf   = build_transforms(img_size, False)\n",
        "\n",
        "    train_ds = datasets.ImageFolder(os.path.join(root, \"train\"), transform=train_tf)\n",
        "    val_ds   = datasets.ImageFolder(os.path.join(root, \"val\"), transform=val_tf)\n",
        "    num_classes = len(train_ds.classes)\n",
        "    print(f\"[CLS] Classes({num_classes}): {train_ds.classes}\")\n",
        "\n",
        "    labelmap_path = os.path.join(weights_dir, \"food_labelmap.json\")\n",
        "    _save_labelmap(train_ds, labelmap_path)\n",
        "    print(f\"[CLS] Saved label map -> {labelmap_path}\")\n",
        "\n",
        "    sampler = _maybe_weighted_sampler(train_ds)\n",
        "    if sampler is None:\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes=num_classes)\n",
        "    if freeze_backbone:\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"classifier\" not in name:\n",
        "                p.requires_grad = False\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = LabelSmoothingCE(eps=label_smoothing)\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs))\n",
        "\n",
        "    best_acc, best_path = 0.0, os.path.join(weights_dir, \"cls_food_best.pth\")\n",
        "    no_improve = 0\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        tc = 0; tt = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tc += (out.argmax(1) == y).sum().item()\n",
        "            tt += x.size(0)\n",
        "        scheduler.step()\n",
        "        train_acc = tc / max(1, tt)\n",
        "\n",
        "        model.eval()\n",
        "        vc = 0; vt = 0\n",
        "        t1s, t3s = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                vc += (out.argmax(1) == y).sum().item()\n",
        "                vt += x.size(0)\n",
        "                t1, t3 = accuracy_topk(out, y, topk=(1,3))\n",
        "                t1s.append(t1.item()); t3s.append(t3.item())\n",
        "        val_acc  = vc / max(1, vt)\n",
        "        val_t1   = float(np.mean(t1s)) if t1s else 0.0\n",
        "        val_t3   = float(np.mean(t3s)) if t3s else 0.0\n",
        "        print(f\"[CLS] Epoch {ep+1:02d}/{epochs} | Train {train_acc:.4f} || Val {val_acc:.4f} Top1 {val_t1:.2f}% Top3 {val_t3:.2f}%\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save({\"model\": model.state_dict(), \"num_classes\": num_classes}, best_path)\n",
        "            print(f\"[CLS] Saved best -> {best_path} (Acc {best_acc:.4f})\")\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(f\"[CLS] Early stopping (patience={patience})\")\n",
        "                break\n",
        "\n",
        "    print(f\"[CLS] Best Val Acc: {best_acc:.4f} | Weights: {best_path}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_food(\n",
        "    img_path: str,\n",
        "    model_path: str = \"weights/cls_food_best.pth\",\n",
        "    labelmap_path: str = \"weights/food_labelmap.json\",\n",
        "    img_size: int = 380,\n",
        "    topk: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    ë¶„ë¥˜ê¸° ì¶”ë¡  (ì‚¬ì§„ -> top-1, top-k í™•ë¥ ).\n",
        "    \"\"\"\n",
        "    device = get_device()\n",
        "    if not os.path.isfile(model_path):\n",
        "        raise FileNotFoundError(model_path)\n",
        "    if not os.path.isfile(labelmap_path):\n",
        "        raise FileNotFoundError(labelmap_path)\n",
        "\n",
        "    labelmap = _load_labelmap(labelmap_path)\n",
        "    num_classes = len(labelmap)\n",
        "\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=False, num_classes=num_classes).to(device)\n",
        "    ckpt = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "    tf = build_transforms(img_size, False)\n",
        "    x = tf(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "\n",
        "    logits = model(x)\n",
        "    probs  = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
        "    k = min(topk, num_classes)\n",
        "    idx = np.argsort(-probs)[:k].tolist()\n",
        "    topk_pairs = [(labelmap[i], float(probs[i])) for i in idx]\n",
        "    return {\"top1\": topk_pairs[0], \"topk\": topk_pairs}\n",
        "\n",
        "# ---------------------\n",
        "# Classifier -> AE verify combo\n",
        "# ---------------------\n",
        "@torch.no_grad()\n",
        "def predict_food_then_verify(\n",
        "    img_path: str,\n",
        "    model_path: str = \"weights/cls_food_best.pth\",\n",
        "    labelmap_path: str = \"weights/food_labelmap.json\",\n",
        "    img_size: int = 380,\n",
        "    topk: int = 3,\n",
        "    verify_with_ae: bool = True,\n",
        "    weights_dir: str = \"weights\",\n",
        "):\n",
        "    \"\"\"\n",
        "    1) ë¶„ë¥˜ê¸° top-k í›„ë³´ ì˜ˆì¸¡\n",
        "    2) (ì˜µì…˜) ê° í›„ë³´ í´ë˜ìŠ¤ì— ëŒ€í•´ AE ì¬êµ¬ì„±ì˜¤ì°¨ë¡œ ê²€ì¦\n",
        "    \"\"\"\n",
        "    pred = predict_food(img_path, model_path, labelmap_path, img_size, topk)\n",
        "    if not verify_with_ae:\n",
        "        return {\"pred\": pred, \"ae_checks\": None}\n",
        "\n",
        "    checks = []\n",
        "    for cname, prob in pred[\"topk\"]:\n",
        "        try:\n",
        "            ok, err, th = is_food_by_ae(img_path, cname, img_size, weights_dir)\n",
        "            checks.append({\"class\": cname, \"prob\": prob, \"ae_ok\": bool(ok), \"recon_error\": err, \"threshold\": th})\n",
        "        except FileNotFoundError:\n",
        "            checks.append({\"class\": cname, \"prob\": prob, \"ae_ok\": None, \"recon_error\": None, \"threshold\": None})\n",
        "    return {\"pred\": pred, \"ae_checks\": checks}\n",
        "\n",
        "# ---------------------\n",
        "#ê¸°ì¡´ 2-í´ë˜ìŠ¤ ë°ëª¨ë„ ìœ ì§€\n",
        "# ---------------------\n",
        "def quick_two_class_demo(\n",
        "    data_dir: str,\n",
        "    img_size: int = 380,\n",
        "    batch_size: int = 32,\n",
        "    epochs: int = 3,\n",
        "    lr: float = 1e-4,\n",
        "):\n",
        "    device = get_device()\n",
        "    train_tf = build_transforms(img_size, True)\n",
        "    val_tf   = build_transforms(img_size, False)\n",
        "\n",
        "    train_ds = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_tf)\n",
        "    val_ds   = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=val_tf)\n",
        "    assert len(train_ds.classes) == 2, \"Need exactly 2 classes for this demo.\"\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes=2).to(device)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt  = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train(); tc=0; tt=0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad(); out = model(x); loss = crit(out, y)\n",
        "            loss.backward(); opt.step()\n",
        "            tc += (out.argmax(1) == y).sum().item()\n",
        "            tt += x.size(0)\n",
        "\n",
        "        model.eval(); vc=0; vt=0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                vc += (out.argmax(1) == y).sum().item()\n",
        "                vt += x.size(0)\n",
        "\n",
        "        print(f\"[2C] Epoch {ep+1}/{epochs} | Train {tc/max(1,tt):.4f} | Val {vc/max(1,vt):.4f}\")\n",
        "    print(\"[2C] Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kaa3Rud80Fb",
        "outputId": "b47bc954-4f9e-4e91-beda-803ea752ee55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì œê±°ëœ Mac ë¦¬ì†ŒìŠ¤ í¬í¬ íŒŒì¼ ìˆ˜: 20004\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "\n",
        "ROOT = \"/content/dataset_split_food20_2\"\n",
        "bad_files = []\n",
        "\n",
        "for f in glob.glob(ROOT + \"/**/._*\", recursive=True):\n",
        "    bad_files.append(f)\n",
        "    os.remove(f)\n",
        "\n",
        "print(\"âœ… ì œê±°ëœ Mac ë¦¬ì†ŒìŠ¤ í¬í¬ íŒŒì¼ ìˆ˜:\", len(bad_files))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KJjH3Yi880IC"
      },
      "outputs": [],
      "source": [
        "import sys, importlib\n",
        "\n",
        "# food20_fullì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ê°•ì œë¡œ ë¦¬ë¡œë“œ\n",
        "if 'food20_full' in sys.modules:\n",
        "    importlib.reload(sys.modules['food20_full'])\n",
        "else:\n",
        "    import food20_full\n",
        "\n",
        "import food20_full as f  # ì•ìœ¼ë¡œ f. ìœ¼ë¡œ í˜¸ì¶œí•  ê±°ì—ìš”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step3. ê²°ê³¼ ì¶œë ¥\n",
        "- ì§œì¥ë©´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í™œìš©í•´ ê²€ì¦"
      ],
      "metadata": {
        "id": "6xjAL7ttzNDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "59144bec41244afd95ebc273e41a6504",
            "d86c489cae864cdbbb4623bde98d5d69",
            "e6baa69370934dedb05d75943ae57a75",
            "2595bbc7e3a846788b661bd948370a55",
            "73e8dd7f9ab4479fa6cc27d481c7b800",
            "3d39e9cea7f6405497cdebba9e9c9397",
            "036e068e29e140acb3852457f067a04b",
            "f0a53c14e9f7454087c501916b4efe45",
            "d56274cb75ae4421a9856b10f183e388",
            "8147fc0b6f5542f297ffc621540c0582",
            "16a713bd46014433994bb405eb7dbff3"
          ]
        },
        "id": "Qbh49Cd_-KxD",
        "outputId": "66d6de95-efde-4075-910f-74876d97c4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Step 1. 20-class ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘\n",
            "[CLS] Classes(20): ['ê°ìì „', 'ê°ìíƒ•', 'ê³¼ë©”ê¸°', 'ë–¡ë³¶ì´', 'ë¼ë©´', 'ë§Œë‘', 'ë³´ìŒˆ', 'ì‚¼ê³„íƒ•', 'ìƒˆìš°íŠ€ê¹€', 'ìˆ˜ì œë¹„', 'ìˆœëŒ€', 'ì–‘ë…ì¹˜í‚¨', 'ìœ¡íšŒ', 'ì¡±ë°œ', 'ì§œì¥ë©´', 'ì§¬ë½•', 'ì½©êµ­ìˆ˜', 'íŒŒì „', 'í”¼ì', 'í›„ë¼ì´ë“œì¹˜í‚¨']\n",
            "[CLS] Saved label map -> weights/food_labelmap.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59144bec41244afd95ebc273e41a6504"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 01/5 | Train 0.7035 || Val 0.8764 Top1 87.64% Top3 97.25%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.8764)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 02/5 | Train 0.9325 || Val 0.9112 Top1 91.12% Top3 98.22%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.9112)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 03/5 | Train 0.9627 || Val 0.9262 Top1 92.62% Top3 98.47%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.9262)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 04/5 | Train 0.9746 || Val 0.9277 Top1 92.78% Top3 98.42%\n",
            "[CLS] Saved best -> weights/cls_food_best.pth (Acc 0.9277)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Epoch 05/5 | Train 0.9796 || Val 0.9257 Top1 92.57% Top3 98.50%\n",
            "[CLS] Best Val Acc: 0.9277 | Weights: weights/cls_food_best.pth\n",
            "\n",
            "ğŸš€ Step 2. í´ë˜ìŠ¤ë³„ AE í•™ìŠµ ì‹œì‘\n",
            "[AE] Target classes: ['ê°ìì „', 'ê°ìíƒ•', 'ê³¼ë©”ê¸°', 'ë–¡ë³¶ì´', 'ë¼ë©´', 'ë§Œë‘', 'ë³´ìŒˆ', 'ì‚¼ê³„íƒ•', 'ìƒˆìš°íŠ€ê¹€', 'ìˆ˜ì œë¹„', 'ìˆœëŒ€', 'ì–‘ë…ì¹˜í‚¨', 'ìœ¡íšŒ', 'ì¡±ë°œ', 'ì§œì¥ë©´', 'ì§¬ë½•', 'ì½©êµ­ìˆ˜', 'íŒŒì „', 'í”¼ì', 'í›„ë¼ì´ë“œì¹˜í‚¨']\n",
            "[AE][ê°ìì „] Epoch 1/3 | Loss 0.004726\n",
            "[AE][ê°ìì „] Epoch 2/3 | Loss 0.003791\n",
            "[AE][ê°ìì „] Epoch 3/3 | Loss 0.003294\n",
            "[AE][ê°ìì „] Saved. threshold=0.005620\n",
            "[AE][ê°ìíƒ•] Epoch 1/3 | Loss 0.004063\n",
            "[AE][ê°ìíƒ•] Epoch 2/3 | Loss 0.003042\n",
            "[AE][ê°ìíƒ•] Epoch 3/3 | Loss 0.002736\n",
            "[AE][ê°ìíƒ•] Saved. threshold=0.004669\n",
            "[AE][ê³¼ë©”ê¸°] Epoch 1/3 | Loss 0.006760\n",
            "[AE][ê³¼ë©”ê¸°] Epoch 2/3 | Loss 0.004999\n",
            "[AE][ê³¼ë©”ê¸°] Epoch 3/3 | Loss 0.004382\n",
            "[AE][ê³¼ë©”ê¸°] Saved. threshold=0.006524\n",
            "[AE][ë–¡ë³¶ì´] Epoch 1/3 | Loss 0.004569\n",
            "[AE][ë–¡ë³¶ì´] Epoch 2/3 | Loss 0.003449\n",
            "[AE][ë–¡ë³¶ì´] Epoch 3/3 | Loss 0.002877\n",
            "[AE][ë–¡ë³¶ì´] Saved. threshold=0.004529\n",
            "[AE][ë¼ë©´] Epoch 1/3 | Loss 0.004176\n",
            "[AE][ë¼ë©´] Epoch 2/3 | Loss 0.003154\n",
            "[AE][ë¼ë©´] Epoch 3/3 | Loss 0.002731\n",
            "[AE][ë¼ë©´] Saved. threshold=0.004838\n",
            "[AE][ë§Œë‘] Epoch 1/3 | Loss 0.005522\n",
            "[AE][ë§Œë‘] Epoch 2/3 | Loss 0.004303\n",
            "[AE][ë§Œë‘] Epoch 3/3 | Loss 0.003681\n",
            "[AE][ë§Œë‘] Saved. threshold=0.005870\n",
            "[AE][ë³´ìŒˆ] Epoch 1/3 | Loss 0.005454\n",
            "[AE][ë³´ìŒˆ] Epoch 2/3 | Loss 0.004211\n",
            "[AE][ë³´ìŒˆ] Epoch 3/3 | Loss 0.003579\n",
            "[AE][ë³´ìŒˆ] Saved. threshold=0.004900\n",
            "[AE][ì‚¼ê³„íƒ•] Epoch 1/3 | Loss 0.004044\n",
            "[AE][ì‚¼ê³„íƒ•] Epoch 2/3 | Loss 0.003053\n",
            "[AE][ì‚¼ê³„íƒ•] Epoch 3/3 | Loss 0.002611\n",
            "[AE][ì‚¼ê³„íƒ•] Saved. threshold=0.004550\n",
            "[AE][ìƒˆìš°íŠ€ê¹€] Epoch 1/3 | Loss 0.005832\n",
            "[AE][ìƒˆìš°íŠ€ê¹€] Epoch 2/3 | Loss 0.004497\n",
            "[AE][ìƒˆìš°íŠ€ê¹€] Epoch 3/3 | Loss 0.003809\n",
            "[AE][ìƒˆìš°íŠ€ê¹€] Saved. threshold=0.005755\n",
            "[AE][ìˆ˜ì œë¹„] Epoch 1/3 | Loss 0.004023\n",
            "[AE][ìˆ˜ì œë¹„] Epoch 2/3 | Loss 0.003126\n",
            "[AE][ìˆ˜ì œë¹„] Epoch 3/3 | Loss 0.002634\n",
            "[AE][ìˆ˜ì œë¹„] Saved. threshold=0.004315\n",
            "[AE][ìˆœëŒ€] Epoch 1/3 | Loss 0.006040\n",
            "[AE][ìˆœëŒ€] Epoch 2/3 | Loss 0.004509\n",
            "[AE][ìˆœëŒ€] Epoch 3/3 | Loss 0.003784\n",
            "[AE][ìˆœëŒ€] Saved. threshold=0.005452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][ì–‘ë…ì¹˜í‚¨] Epoch 1/3 | Loss 0.004652\n",
            "[AE][ì–‘ë…ì¹˜í‚¨] Epoch 2/3 | Loss 0.003453\n",
            "[AE][ì–‘ë…ì¹˜í‚¨] Epoch 3/3 | Loss 0.002877\n",
            "[AE][ì–‘ë…ì¹˜í‚¨] Saved. threshold=0.004694\n",
            "[AE][ìœ¡íšŒ] Epoch 1/3 | Loss 0.005792\n",
            "[AE][ìœ¡íšŒ] Epoch 2/3 | Loss 0.004336\n",
            "[AE][ìœ¡íšŒ] Epoch 3/3 | Loss 0.003556\n",
            "[AE][ìœ¡íšŒ] Saved. threshold=0.005325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][ì¡±ë°œ] Epoch 1/3 | Loss 0.004970\n",
            "[AE][ì¡±ë°œ] Epoch 2/3 | Loss 0.003750\n",
            "[AE][ì¡±ë°œ] Epoch 3/3 | Loss 0.003118\n",
            "[AE][ì¡±ë°œ] Saved. threshold=0.004530\n",
            "[AE][ì§œì¥ë©´] Epoch 1/3 | Loss 0.005458\n",
            "[AE][ì§œì¥ë©´] Epoch 2/3 | Loss 0.003703\n",
            "[AE][ì§œì¥ë©´] Epoch 3/3 | Loss 0.003009\n",
            "[AE][ì§œì¥ë©´] Saved. threshold=0.004651\n",
            "[AE][ì§¬ë½•] Epoch 1/3 | Loss 0.003411\n",
            "[AE][ì§¬ë½•] Epoch 2/3 | Loss 0.002409\n",
            "[AE][ì§¬ë½•] Epoch 3/3 | Loss 0.002082\n",
            "[AE][ì§¬ë½•] Saved. threshold=0.003677\n",
            "[AE][ì½©êµ­ìˆ˜] Epoch 1/3 | Loss 0.004975\n",
            "[AE][ì½©êµ­ìˆ˜] Epoch 2/3 | Loss 0.003930\n",
            "[AE][ì½©êµ­ìˆ˜] Epoch 3/3 | Loss 0.003356\n",
            "[AE][ì½©êµ­ìˆ˜] Saved. threshold=0.005261\n",
            "[AE][íŒŒì „] Epoch 1/3 | Loss 0.004219\n",
            "[AE][íŒŒì „] Epoch 2/3 | Loss 0.003178\n",
            "[AE][íŒŒì „] Epoch 3/3 | Loss 0.002673\n",
            "[AE][íŒŒì „] Saved. threshold=0.004471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][í”¼ì] Epoch 1/3 | Loss 0.002910\n",
            "[AE][í”¼ì] Epoch 2/3 | Loss 0.002235\n",
            "[AE][í”¼ì] Epoch 3/3 | Loss 0.001972\n",
            "[AE][í”¼ì] Saved. threshold=0.004085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE][í›„ë¼ì´ë“œì¹˜í‚¨] Epoch 1/3 | Loss 0.004538\n",
            "[AE][í›„ë¼ì´ë“œì¹˜í‚¨] Epoch 2/3 | Loss 0.003426\n",
            "[AE][í›„ë¼ì´ë“œì¹˜í‚¨] Epoch 3/3 | Loss 0.002987\n",
            "[AE][í›„ë¼ì´ë“œì¹˜í‚¨] Saved. threshold=0.004971\n",
            "í´ë˜ìŠ¤ë³„ Thresholds: {'ê°ìì „': 0.005619973875582218, 'ê°ìíƒ•': 0.004669167101383209, 'ê³¼ë©”ê¸°': 0.006523739546537399, 'ë–¡ë³¶ì´': 0.004529417492449284, 'ë¼ë©´': 0.004837975837290287, 'ë§Œë‘': 0.005870405584573746, 'ë³´ìŒˆ': 0.004900071769952774, 'ì‚¼ê³„íƒ•': 0.004549554083496332, 'ìƒˆìš°íŠ€ê¹€': 0.0057545071467757225, 'ìˆ˜ì œë¹„': 0.004315010737627745, 'ìˆœëŒ€': 0.005451973527669907, 'ì–‘ë…ì¹˜í‚¨': 0.004694024566560984, 'ìœ¡íšŒ': 0.005324722267687321, 'ì¡±ë°œ': 0.004530000500380993, 'ì§œì¥ë©´': 0.00465080002322793, 'ì§¬ë½•': 0.003676858264952898, 'ì½©êµ­ìˆ˜': 0.005260597914457321, 'íŒŒì „': 0.004470800515264273, 'í”¼ì': 0.004085250664502382, 'í›„ë¼ì´ë“œì¹˜í‚¨': 0.0049706390127539635}\n",
            "\n",
            "ğŸš€ Step 3. ì¶”ë¡  ì‹¤í–‰\n",
            "\n",
            "ìµœì¢… ê²°ê³¼:\n",
            "{'pred': {'top1': ('ì§œì¥ë©´', 0.8835874795913696), 'topk': [('ì§œì¥ë©´', 0.8835874795913696), ('í›„ë¼ì´ë“œì¹˜í‚¨', 0.012351472862064838), ('ê°ìì „', 0.010002527385950089)]}, 'ae_checks': [{'class': 'ì§œì¥ë©´', 'prob': 0.8835874795913696, 'ae_ok': True, 'recon_error': 0.001568456063978374, 'threshold': 0.00465080002322793}, {'class': 'í›„ë¼ì´ë“œì¹˜í‚¨', 'prob': 0.012351472862064838, 'ae_ok': False, 'recon_error': 0.006881140638142824, 'threshold': 0.0049706390127539635}, {'class': 'ê°ìì „', 'prob': 0.010002527385950089, 'ae_ok': False, 'recon_error': 0.005914350505918264, 'threshold': 0.005619973875582218}]}\n"
          ]
        }
      ],
      "source": [
        "import sys, importlib\n",
        "\n",
        "# ëª¨ë“ˆ ë¦¬ë¡œë“œ\n",
        "if \"food20_full\" in sys.modules:\n",
        "    importlib.reload(sys.modules[\"food20_full\"])\n",
        "else:\n",
        "    import food20_full\n",
        "\n",
        "import food20_full as f   # ì•ìœ¼ë¡œ f. ìœ¼ë¡œ í˜¸ì¶œ\n",
        "\n",
        "# 1. ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ\n",
        "ROOT = \"/content/dataset_split_food20_2\"\n",
        "\n",
        "# 2. ë¶„ë¥˜ê¸° í•™ìŠµ\n",
        "print(\"ğŸš€ Step 1. 20-class ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘\")\n",
        "f.train_food_classifier(\n",
        "    root=ROOT,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        "    label_smoothing=0.1,\n",
        "    freeze_backbone=False\n",
        ")\n",
        "\n",
        "# 3. í´ë˜ìŠ¤ë³„ AE í•™ìŠµ\n",
        "print(\"\\nğŸš€ Step 2. í´ë˜ìŠ¤ë³„ AE í•™ìŠµ ì‹œì‘\")\n",
        "thresholds = f.train_many_oneclass_aes(\n",
        "    root=ROOT,\n",
        "    epochs=3\n",
        ")\n",
        "print(\"í´ë˜ìŠ¤ë³„ Thresholds:\", thresholds)\n",
        "\n",
        "# 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "print(\"\\nğŸš€ Step 3. ì¶”ë¡  ì‹¤í–‰\")\n",
        "test_img = \"/content/common-9.jpg\"\n",
        "result = f.predict_food_then_verify(\n",
        "    img_path=test_img,\n",
        "    model_path=\"weights/cls_food_best.pth\",\n",
        "    labelmap_path=\"weights/food_labelmap.json\",\n",
        "    img_size=380,\n",
        "    topk=3,\n",
        "    verify_with_ae=True\n",
        ")\n",
        "\n",
        "print(\"\\nìµœì¢… ê²°ê³¼:\")\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59144bec41244afd95ebc273e41a6504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d86c489cae864cdbbb4623bde98d5d69",
              "IPY_MODEL_e6baa69370934dedb05d75943ae57a75",
              "IPY_MODEL_2595bbc7e3a846788b661bd948370a55"
            ],
            "layout": "IPY_MODEL_73e8dd7f9ab4479fa6cc27d481c7b800"
          }
        },
        "d86c489cae864cdbbb4623bde98d5d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d39e9cea7f6405497cdebba9e9c9397",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_036e068e29e140acb3852457f067a04b",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "e6baa69370934dedb05d75943ae57a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a53c14e9f7454087c501916b4efe45",
            "max": 77933206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d56274cb75ae4421a9856b10f183e388",
            "value": 77933206
          }
        },
        "2595bbc7e3a846788b661bd948370a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8147fc0b6f5542f297ffc621540c0582",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_16a713bd46014433994bb405eb7dbff3",
            "value": "â€‡77.9M/77.9Mâ€‡[00:00&lt;00:00,â€‡93.9MB/s]"
          }
        },
        "73e8dd7f9ab4479fa6cc27d481c7b800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d39e9cea7f6405497cdebba9e9c9397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "036e068e29e140acb3852457f067a04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0a53c14e9f7454087c501916b4efe45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56274cb75ae4421a9856b10f183e388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8147fc0b6f5542f297ffc621540c0582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16a713bd46014433994bb405eb7dbff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}